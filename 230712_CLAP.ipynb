{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFtHEaHjwjQLMICgbrvVV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunMin-Ju/IIP/blob/main/230712_CLAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_nj0sD3UWRI",
        "outputId": "8d4b6e2c-9b8b-4dbc-be46-b0c3a088bbd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK9yieE5TGcT",
        "outputId": "835e5d4f-040c-4888-98ee-44dc5cd97a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 1)) (1.4.4)\n",
            "Collecting audioread==2.1.9 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 2))\n",
            "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.5/377.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting certifi==2020.12.5 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 3))\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cffi==1.14.5 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 4))\n",
            "  Downloading cffi-1.14.5.tar.gz (475 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.1/475.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 5)) (4.0.0)\n",
            "Collecting click==7.1.2 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 6))\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting configparser==5.0.2 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 7))\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting cycler==0.10.0 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 8))\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting decorator==5.0.7 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 9))\n",
            "  Downloading decorator-5.0.7-py3-none-any.whl (8.8 kB)\n",
            "Collecting docker-pycreds==0.4.0 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 10))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting filelock==3.0.12 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 11))\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting gitdb==4.0.7 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 12))\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython==3.1.14 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 13))\n",
            "  Downloading GitPython-3.1.14-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.8/159.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py==3.2.1 (from -r /content/drive/MyDrive/CLAP/requirements.txt (line 14))\n",
            "  Downloading h5py-3.2.1.tar.gz (368 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.2/368.2 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "pip install -r /content/drive/MyDrive/CLAP/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CLAP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm0ZI1gjWLYm",
        "outputId": "86e1e978-74b9-4308-8d93-361cd3db0a63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CLAP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RlJhzeUWC1e",
        "outputId": "228416a6-765e-4675-dca0-fe0b1750552d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CLAP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nogMv7LZWINO",
        "outputId": "4b2a3cfb-1d21-443c-9950-eeff24ed6032"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio.py  CLAP_weights_2022.pth  data_path    requirements.txt\n",
            "clap.py   config.yml\t\t __pycache__  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeSY9GFPWtql",
        "outputId": "dca295bb-4ebb-4ff9-9cbc-50f2391c4b12"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##src/esc50_dataset.py\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets.utils import download_url\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, root: str, download: bool = True):\n",
        "        self.root = os.path.expanduser(root)\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def download(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class ESC50(AudioDataset):\n",
        "    base_folder = 'ESC-50-master'\n",
        "    url = \"https://github.com/karolpiczak/ESC-50/archive/refs/heads/master.zip\"\n",
        "    filename = \"ESC-50-master.zip\"\n",
        "    num_files_in_dir = 2000\n",
        "    audio_dir = 'audio'\n",
        "    label_col = 'category'\n",
        "    file_col = 'filename'\n",
        "    meta = {\n",
        "        'filename': os.path.join('meta','esc50.csv'),\n",
        "    }\n",
        "\n",
        "    def __init__(self, root, reading_transformations: nn.Module = None, download: bool = True):\n",
        "        super().__init__(root)\n",
        "        self._load_meta()\n",
        "\n",
        "        self.targets, self.audio_paths = [], []\n",
        "        self.pre_transformations = reading_transformations\n",
        "        print(\"Loading audio files\")\n",
        "        # self.df['filename'] = os.path.join(self.root, self.base_folder, self.audio_dir) + os.sep + self.df['filename']\n",
        "        self.df['category'] = self.df['category'].str.replace('_',' ')\n",
        "\n",
        "        for _, row in tqdm(self.df.iterrows()):\n",
        "            file_path = os.path.join(self.root, self.base_folder, self.audio_dir, row[self.file_col])\n",
        "            self.targets.append(row[self.label_col])\n",
        "            self.audio_paths.append(file_path)\n",
        "\n",
        "    def _load_meta(self):\n",
        "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
        "\n",
        "        self.df = pd.read_csv(path)\n",
        "        self.class_to_idx = {}\n",
        "        self.classes = [x.replace('_',' ') for x in sorted(self.df[self.label_col].unique())]\n",
        "        for i, category in enumerate(self.classes):\n",
        "            self.class_to_idx[category] = i\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        file_path, target = self.audio_paths[index], self.targets[index]\n",
        "        idx = torch.tensor(self.class_to_idx[target])\n",
        "        one_hot_target = torch.zeros(len(self.classes)).scatter_(0, idx, 1).reshape(1,-1)\n",
        "        return file_path, target, one_hot_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_paths)\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.url, self.root, self.filename)\n",
        "\n",
        "        # extract file\n",
        "        from zipfile import ZipFile\n",
        "        with ZipFile(os.path.join(self.root, self.filename), 'r') as zip:\n",
        "            zip.extractall(path=self.root)"
      ],
      "metadata": {
        "id": "YQDaxeJdXgxP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#src/CLAPWrapper.py\n",
        "import random\n",
        "import torchaudio\n",
        "from torch import inf\n",
        "import collections\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import math\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import torch\n",
        "from importlib_resources import files\n",
        "\n",
        "\n",
        "class CLAPWrapper():\n",
        "    \"\"\"\n",
        "    A class for interfacing CLAP model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_fp, use_cuda=False):\n",
        "        self.np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
        "        self.file_path = os.path.dirname(os.path.abspath('__file__'))\n",
        "        self.default_collate_err_msg_format = (\n",
        "            \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n",
        "            \"dicts or lists; found {}\")\n",
        "        f = pathlib.Path(\"/content/drive/MyDrive/CLAP/config.yml\")\n",
        "        self.config_as_str = f.read_text()\n",
        "        self.model_fp = model_fp\n",
        "        self.use_cuda = use_cuda\n",
        "        self.clap, self.tokenizer, self.args = self.load_clap()\n",
        "\n",
        "    def load_clap(self):\n",
        "        r\"\"\"Load CLAP model with args from config file\"\"\"\n",
        "\n",
        "        args = read_config_as_args(self.config_as_str, is_config_str=True)\n",
        "\n",
        "        if 'bert' in args.text_model:\n",
        "            self.token_keys = ['input_ids', 'token_type_ids', 'attention_mask']\n",
        "        else:\n",
        "            self.token_keys = ['input_ids', 'attention_mask']\n",
        "\n",
        "        clap = CLAP(\n",
        "            audioenc_name=args.audioenc_name,\n",
        "            sample_rate=args.sampling_rate,\n",
        "            window_size=args.window_size,\n",
        "            hop_size=args.hop_size,\n",
        "            mel_bins=args.mel_bins,\n",
        "            fmin=args.fmin,\n",
        "            fmax=args.fmax,\n",
        "            classes_num=args.num_classes,\n",
        "            out_emb=args.out_emb,\n",
        "            text_model=args.text_model,\n",
        "            transformer_embed_dim=args.transformer_embed_dim,\n",
        "            d_proj=args.d_proj\n",
        "        )\n",
        "\n",
        "        # Load pretrained weights for model\n",
        "        model_state_dict = torch.load(self.model_fp, map_location=torch.device('cpu'))['model']\n",
        "        clap.load_state_dict(model_state_dict)\n",
        "\n",
        "        clap.eval()  # set clap in eval mode\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.text_model)\n",
        "\n",
        "        if self.use_cuda and torch.cuda.is_available():\n",
        "            clap = clap.cuda()\n",
        "\n",
        "        return clap, tokenizer, args\n",
        "\n",
        "    def default_collate(self, batch):\n",
        "        r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
        "        elem = batch[0]\n",
        "        elem_type = type(elem)\n",
        "        if isinstance(elem, torch.Tensor):\n",
        "            out = None\n",
        "            if torch.utils.data.get_worker_info() is not None:\n",
        "                # If we're in a background process, concatenate directly into a\n",
        "                # shared memory tensor to avoid an extra copy\n",
        "                numel = sum([x.numel() for x in batch])\n",
        "                storage = elem.storage()._new_shared(numel)\n",
        "                out = elem.new(storage)\n",
        "            return torch.stack(batch, 0, out=out)\n",
        "        elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
        "                and elem_type.__name__ != 'string_':\n",
        "            if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
        "                # array of string classes and object\n",
        "                if self.np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
        "                    raise TypeError(\n",
        "                        self.default_collate_err_msg_format.format(elem.dtype))\n",
        "\n",
        "                return self.default_collate([torch.as_tensor(b) for b in batch])\n",
        "            elif elem.shape == ():  # scalars\n",
        "                return torch.as_tensor(batch)\n",
        "        elif isinstance(elem, float):\n",
        "            return torch.tensor(batch, dtype=torch.float64)\n",
        "        elif isinstance(elem, int):\n",
        "            return torch.tensor(batch)\n",
        "        elif isinstance(elem, str):\n",
        "            return batch\n",
        "        elif isinstance(elem, collections.abc.Mapping):\n",
        "            return {key: self.default_collate([d[key] for d in batch]) for key in elem}\n",
        "        elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
        "            return elem_type(*(self.default_collate(samples) for samples in zip(*batch)))\n",
        "        elif isinstance(elem, collections.abc.Sequence):\n",
        "            # check to make sure that the elements in batch have consistent size\n",
        "            it = iter(batch)\n",
        "            elem_size = len(next(it))\n",
        "            if not all(len(elem) == elem_size for elem in it):\n",
        "                raise RuntimeError(\n",
        "                    'each element in list of batch should be of equal size')\n",
        "            transposed = zip(*batch)\n",
        "            return [self.default_collate(samples) for samples in transposed]\n",
        "\n",
        "        raise TypeError(self.default_collate_err_msg_format.format(elem_type))\n",
        "\n",
        "    def load_audio_into_tensor(self, audio_path, audio_duration, resample=False):\n",
        "        r\"\"\"Loads audio file and returns raw audio.\"\"\"\n",
        "        # Randomly sample a segment of audio_duration from the clip or pad to match duration\n",
        "        audio_time_series, sample_rate = torchaudio.load(audio_path)\n",
        "        resample_rate = self.args.sampling_rate\n",
        "        if resample:\n",
        "            resampler = T.Resample(sample_rate, resample_rate)\n",
        "            audio_time_series = resampler(audio_time_series)\n",
        "        audio_time_series = audio_time_series.reshape(-1)\n",
        "\n",
        "        # audio_time_series is shorter than predefined audio duration,\n",
        "        # so audio_time_series is extended\n",
        "        if audio_duration*sample_rate >= audio_time_series.shape[0]:\n",
        "            repeat_factor = int(np.ceil((audio_duration*sample_rate) /\n",
        "                                        audio_time_series.shape[0]))\n",
        "            # Repeat audio_time_series by repeat_factor to match audio_duration\n",
        "            audio_time_series = audio_time_series.repeat(repeat_factor)\n",
        "            # remove excess part of audio_time_series\n",
        "            audio_time_series = audio_time_series[0:audio_duration*sample_rate]\n",
        "        else:\n",
        "            # audio_time_series is longer than predefined audio duration,\n",
        "            # so audio_time_series is trimmed\n",
        "            start_index = random.randrange(\n",
        "                audio_time_series.shape[0] - audio_duration*sample_rate)\n",
        "            audio_time_series = audio_time_series[start_index:start_index +\n",
        "                                                  audio_duration*sample_rate]\n",
        "        return torch.FloatTensor(audio_time_series)\n",
        "\n",
        "    def preprocess_audio(self, audio_files, resample):\n",
        "        r\"\"\"Load list of audio files and return raw audio\"\"\"\n",
        "        audio_tensors = []\n",
        "        for audio_file in audio_files:\n",
        "            audio_tensor = self.load_audio_into_tensor(\n",
        "                audio_file, self.args.duration, resample)\n",
        "            audio_tensor = audio_tensor.reshape(\n",
        "                1, -1).cuda() if self.use_cuda and torch.cuda.is_available() else audio_tensor.reshape(1, -1)\n",
        "            audio_tensors.append(audio_tensor)\n",
        "        return self.default_collate(audio_tensors)\n",
        "\n",
        "    def preprocess_text(self, text_queries):\n",
        "        r\"\"\"Load list of class labels and return tokenized text\"\"\"\n",
        "        tokenized_texts = []\n",
        "        for ttext in text_queries:\n",
        "            tok = self.tokenizer.encode_plus(\n",
        "                text=ttext, add_special_tokens=True, max_length=self.args.text_len, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "            for key in self.token_keys:\n",
        "                tok[key] = tok[key].reshape(-1).cuda() if self.use_cuda and torch.cuda.is_available() else tok[key].reshape(-1)\n",
        "            tokenized_texts.append(tok)\n",
        "        return self.default_collate(tokenized_texts)\n",
        "\n",
        "    def get_text_embeddings(self, class_labels):\n",
        "        r\"\"\"Load list of class labels and return text embeddings\"\"\"\n",
        "        preprocessed_text = self.preprocess_text(class_labels)\n",
        "        text_embeddings = self._get_text_embeddings(preprocessed_text)\n",
        "        text_embeddings = text_embeddings/torch.norm(text_embeddings, dim=-1, keepdim=True)\n",
        "        return text_embeddings\n",
        "\n",
        "    def get_audio_embeddings(self, audio_files, resample):\n",
        "        r\"\"\"Load list of audio files and return a audio embeddings\"\"\"\n",
        "        preprocessed_audio = self.preprocess_audio(audio_files, resample)\n",
        "        audio_embeddings = self._get_audio_embeddings(preprocessed_audio)\n",
        "        audio_embeddings = audio_embeddings/torch.norm(audio_embeddings, dim=-1, keepdim=True)\n",
        "        return audio_embeddings\n",
        "\n",
        "    def _get_text_embeddings(self, preprocessed_text):\n",
        "        r\"\"\"Load preprocessed text and return text embeddings\"\"\"\n",
        "        with torch.no_grad():\n",
        "            text_embeddings = self.clap.caption_encoder(preprocessed_text)\n",
        "            text_embeddings = text_embeddings/torch.norm(text_embeddings, dim=-1, keepdim=True)\n",
        "            return text_embeddings\n",
        "\n",
        "    def _get_audio_embeddings(self, preprocessed_audio):\n",
        "        r\"\"\"Load preprocessed audio and return a audio embeddings\"\"\"\n",
        "        with torch.no_grad():\n",
        "            preprocessed_audio = preprocessed_audio.reshape(\n",
        "                preprocessed_audio.shape[0], preprocessed_audio.shape[2])\n",
        "            #Append [0] the audio emebdding, [1] has output class probabilities\n",
        "            audio_embeddings = self.clap.audio_encoder(preprocessed_audio)[0]\n",
        "            audio_embeddings = audio_embeddings/torch.norm(audio_embeddings, dim=-1, keepdim=True)\n",
        "            return audio_embeddings\n",
        "\n",
        "    def compute_similarity(self, audio_embeddings, text_embeddings):\n",
        "        r\"\"\"Compute similarity between text and audio embeddings\"\"\"\n",
        "        logit_scale = self.clap.logit_scale.exp()\n",
        "        similarity = logit_scale*text_embeddings @ audio_embeddings.T\n",
        "        return similarity.T\n",
        "\n",
        "    def _generic_batch_inference(self, func, *args):\n",
        "        r\"\"\"Process audio and/or text per batch\"\"\"\n",
        "        input_tmp = args[0]\n",
        "        batch_size = args[-1]\n",
        "        # args[0] has audio_files, args[1] has class_labels\n",
        "        inputs = [args[0], args[1]] if len(args) == 3 else [args[0]]\n",
        "        args0_len = len(args[0])\n",
        "        # compute text_embeddings once for all the audio_files batches\n",
        "        if len(inputs) == 2:\n",
        "            text_embeddings = self.get_text_embeddings(args[1])\n",
        "            inputs = [args[0], args[1], text_embeddings]\n",
        "        dataset_idx = 0\n",
        "        for _ in range(math.ceil(args0_len/batch_size)):\n",
        "            next_batch_idx = dataset_idx + batch_size\n",
        "            # batch size is bigger than available audio/text items\n",
        "            if next_batch_idx >= args0_len:\n",
        "                inputs[0] = input_tmp[dataset_idx:]\n",
        "                return func(*tuple(inputs))\n",
        "            else:\n",
        "                inputs[0] = input_tmp[dataset_idx:next_batch_idx]\n",
        "                yield func(*tuple(inputs))\n",
        "            dataset_idx = next_batch_idx\n",
        "\n",
        "    def get_audio_embeddings_per_batch(self, audio_files, batch_size):\n",
        "        r\"\"\"Load preprocessed audio and return a audio embeddings per batch\"\"\"\n",
        "        return self._generic_batch_inference(self.get_audio_embeddings, audio_files, batch_size)\n",
        "\n",
        "    def get_text_embeddings_per_batch(self, class_labels, batch_size):\n",
        "        r\"\"\"Load preprocessed text and return text embeddings per batch\"\"\"\n",
        "        return self._generic_batch_inference(self.get_text_embeddings, class_labels, batch_size)\n",
        "\n",
        "    def classify_audio_files_per_batch(self, audio_files, class_labels, batch_size):\n",
        "        r\"\"\"Compute classification probabilities for each audio recording in a batch and each class label\"\"\"\n",
        "        return self._generic_batch_inference(self.classify_audio_files, audio_files, class_labels, batch_size)"
      ],
      "metadata": {
        "id": "m3TbhbMfW-OW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchlibrosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLYotgS1YFAx",
        "outputId": "62f94cd0-1b7a-406c-bdbe-75e0a376ee42"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchlibrosa\n",
            "  Downloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchlibrosa) (1.22.4)\n",
            "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchlibrosa) (0.10.0.post2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (4.6.3)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.8.0->torchlibrosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.8.0->torchlibrosa) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa>=0.8.0->torchlibrosa) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa>=0.8.0->torchlibrosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa>=0.8.0->torchlibrosa) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa>=0.8.0->torchlibrosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa>=0.8.0->torchlibrosa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa>=0.8.0->torchlibrosa) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa>=0.8.0->torchlibrosa) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa>=0.8.0->torchlibrosa) (3.4)\n",
            "Installing collected packages: torchlibrosa\n",
            "Successfully installed torchlibrosa-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import audio\n",
        "import utils\n",
        "\n",
        "from utils import read_config_as_args\n"
      ],
      "metadata": {
        "id": "A6PAc5CDTRAx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##clap.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "from audio import get_audio_encoder\n",
        "\n",
        "class Projection(nn.Module):\n",
        "    def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.linear2 = nn.Linear(d_out, d_out, bias=False)\n",
        "        self.layer_norm = nn.LayerNorm(d_out)\n",
        "        self.drop = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        embed1 = self.linear1(x)\n",
        "        embed2 = self.drop(self.linear2(F.gelu(embed1)))\n",
        "        embeds = self.layer_norm(embed1 + embed2)\n",
        "        return embeds\n",
        "\n",
        "class AudioEncoder(nn.Module):\n",
        "    def __init__(self, audioenc_name:str, d_in: int, d_out: int, sample_rate: int, window_size: int,\n",
        "            hop_size: int, mel_bins: int, fmin: int, fmax: int, classes_num: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        audio_encoder = get_audio_encoder(audioenc_name)\n",
        "\n",
        "        self.base = audio_encoder(\n",
        "            sample_rate, window_size,\n",
        "            hop_size, mel_bins, fmin, fmax,\n",
        "            classes_num, d_in)\n",
        "\n",
        "        self.projection = Projection(d_in, d_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_dict = self.base(x)\n",
        "        audio_features, audio_classification_output = out_dict['embedding'], out_dict['clipwise_output']\n",
        "        projected_vec = self.projection(audio_features)\n",
        "        return projected_vec, audio_classification_output\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, d_out: int, text_model: str, transformer_embed_dim: int) -> None:\n",
        "        super().__init__()\n",
        "        self.base = AutoModel.from_pretrained(text_model)\n",
        "\n",
        "        self.projection = Projection(transformer_embed_dim, d_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.base(**x)[0]\n",
        "        out = out[:, 0, :]  # get CLS token output\n",
        "        projected_vec = self.projection(out)\n",
        "        return projected_vec\n",
        "\n",
        "class CLAP(nn.Module):\n",
        "    def __init__(self,\n",
        "                # audio\n",
        "                audioenc_name: str,\n",
        "                sample_rate: int,\n",
        "                window_size: int,\n",
        "                hop_size: int,\n",
        "                mel_bins: int,\n",
        "                fmin: int,\n",
        "                fmax: int,\n",
        "                classes_num: int,\n",
        "                out_emb: int,\n",
        "                # text\n",
        "                text_model: str,\n",
        "                transformer_embed_dim: int,\n",
        "                # common\n",
        "                d_proj: int,\n",
        "                ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.audio_encoder = AudioEncoder(\n",
        "            audioenc_name, out_emb, d_proj,\n",
        "            sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num)\n",
        "\n",
        "        self.caption_encoder = TextEncoder(\n",
        "            d_proj, text_model, transformer_embed_dim\n",
        "        )\n",
        "\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "    def forward(self, audio, text):\n",
        "        audio_embed, _ = self.audio_encoder(audio)\n",
        "        caption_embed = self.caption_encoder(text)\n",
        "\n",
        "        return caption_embed, audio_embed, self.logit_scale.exp()"
      ],
      "metadata": {
        "id": "xdyCX2OuY73L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install configs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16zQeqQMc6jN",
        "outputId": "02d6dbd3-0ce9-4546-b7e5-2557c14f6e3d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting configs\n",
            "  Downloading configs-3.0.3-py3-none-any.whl (7.1 kB)\n",
            "Installing collected packages: configs\n",
            "Successfully installed configs-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install subprocess.run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mztD5jXfhXc",
        "outputId": "1837d7c9-df1e-403c-8e4c-54058516cb7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting subprocess.run\n",
            "  Downloading subprocess.run-0.0.8.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: subprocess.run\n",
            "  Building wheel for subprocess.run (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess.run: filename=subprocess.run-0.0.8-py3-none-any.whl size=5345 sha256=aa4e15043b7cfbf3289880b0ad3ea51752c9345d2a434a35f60fd7eeddf91f9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/54/f9/27472395b7889f9ee5a982bbabb6e9310b59cdc6e7e45bdd87\n",
            "Successfully built subprocess.run\n",
            "Installing collected packages: subprocess.run\n",
            "Successfully installed subprocess.run-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is an example using CLAP to perform zeroshot\n",
        "    classification on ESC50 (https://github.com/karolpiczak/ESC-50).\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "dataset = ESC50(root=\"data_path\", download=False)\n",
        "prompt = 'this is a sound of '\n",
        "y = [prompt + x for x in dataset.classes]\n",
        "\n",
        "\n",
        "# Load and initialize CLAP\n",
        "weights_path = \"/content/drive/MyDrive/CLAP/CLAP_weights_2022.pth\"\n",
        "\n",
        "clap_model = CLAPWrapper(weights_path,use_cuda=False)\n",
        "\n",
        "\n",
        "# Computing text embeddings\n",
        "text_embeddings = clap_model.get_text_embeddings(y)\n",
        "\n",
        "# Computing audio embeddings\n",
        "y_preds, y_labels = [], []\n",
        "for i in tqdm(range(len(dataset))):\n",
        "    x, _, one_hot_target = dataset.__getitem__(i)\n",
        "    audio_embeddings = clap_model.get_audio_embeddings([x], resample=True)\n",
        "    similarity = clap_model.compute_similarity(audio_embeddings, text_embeddings)\n",
        "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
        "    y_preds.append(y_pred)\n",
        "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
        "\n",
        "y_labels, y_preds = np.concatenate(y_labels, axis=0), np.concatenate(y_preds, axis=0)\n",
        "acc = accuracy_score(np.argmax(y_labels, axis=1), np.argmax(y_preds, axis=1))\n",
        "print('ESC50 Accuracy {}'.format(acc))\n",
        "\n",
        "\"\"\"\n",
        "The output:\n",
        "\n",
        "ESC50 Accuracy: 82.6%\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "euVUIa2PWWq8",
        "outputId": "1a75c73e-3804-4d63-e575-8a5d99959337"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: data_path/ESC-50-master.zip\n",
            "Loading audio files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [00:00, 9425.00it/s] \n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 2000/2000 [32:50<00:00,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ESC50 Accuracy 0.8265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe output:\\n\\nESC50 Accuracy: 82.6%\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}